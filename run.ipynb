{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run on colab\n",
    "%%capture\n",
    "!git clone https://github.com/traeuker/CreateGPT.git\n",
    "%cd /content/CreateGPT\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CreateGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_model\n",
    "import sample\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained weights \n",
    "\n",
    "You need a model with the same architecture as the pre-trained model\n",
    "\n",
    "You also need the same tokenizer as the pre-trained model to fit vocab size and special tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_weights\n",
    "\n",
    "transformer_config = gpt_model.TransformerConfig(\n",
    "    num_layers=12,\n",
    "    num_heads=12,\n",
    "    vocab_size=50257,\n",
    "    hidden_size=768,\n",
    "    max_seq_len=1024,\n",
    "    dropout=0.1,\n",
    "    layer_norm_epsilon=1e-5\n",
    ")\n",
    "\n",
    "model = gpt_model.DecoderOnlyTransformer(config=transformer_config)\n",
    "model = load_weights.copy_weights(model)\n",
    "\n",
    "tokenizer = load_weights.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My research interests touch several areas of Machine Learning, Signal Processing, and Machine Learning as well as the development and implementation of Artificial Intelligence, which are all aspects of our research. As such, we are working on many\n"
     ]
    }
   ],
   "source": [
    "# test if weights were copied correctly\n",
    "text = \"My research interests touch several areas of Machine Learning, Signal Processing,\"\n",
    "output = sample.sample_tokens(model=model, initial_text=text, tokenizer=tokenizer, max_tokens_generated=30, temperature=1.0, top_k=20)\n",
    "print(output)\n",
    "# Does the output look like a somewhat reasonable continuation of the input text?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model\n",
    "\n",
    "You can train your model with your own dataset!\n",
    "\n",
    "You only need a text file with enough data (and ideally enough compute that you are not limited by training time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from urllib.request import urlopen\n",
    "import data_processing\n",
    "import load_weights\n",
    "\n",
    "# Url to a text file \n",
    "url = \"https://www.gutenberg.org/cache/epub/100/pg100.txt\"\n",
    "data = urlopen(url).read().decode('utf-8')\n",
    "data = \" \".join(data.split())\n",
    "\n",
    "# fraction of the dataset to use, using all of it may take a long time\n",
    "fraction_of_dataset = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either load the dataset with the GPT2 tokenizer\n",
    "tokenizer = load_weights.get_tokenizer()\n",
    "dataset = data_processing.WordsDatasetTokenized(dataset=data, tokenizer=tokenizer, fraction=fraction_of_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or create your own dataset and create your tokenizer from it \n",
    "import re\n",
    "words = re.split(r\"\\b\", data)\n",
    "dataset = data_processing.WordsDataset(words=words, fraction=fraction_of_dataset)\n",
    "tokenizer = data_processing.WordsTokenizer(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training Loss 12.6517: 100%|██████████| 291/291 [1:57:17<00:00, 24.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 34.9738\n",
      "Test Accuracy: 0.1294\n",
      "Training complete in 117m 28s\n"
     ]
    }
   ],
   "source": [
    "# train model on the dataset\n",
    "model = gpt_model.DecoderOnlyTransformer(config=transformer_config)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "trainloader, testloader = data_processing.get_dataloaders(\n",
    "    dataset, batch_size=4)\n",
    "trained_model = gpt_model.train(model, optimizer, trainloader, testloader, loss_fn, num_epochs=1,\n",
    "                                save_dir=\"/Users/tilman/Documents/projects/arena/arena/CreateGPT2/CreateGPT/models\", \n",
    "                                device=device, WANDB=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if training worked\n",
    "text = \"My research interests touch several areas of Machine Learning, Signal Processing \"\n",
    "output = sample.sample_tokens(model=trained_model, initial_text=text, tokenizer=tokenizer, max_tokens_generated=30, temperature=0.9, top_k=20)\n",
    "print(output)\n",
    "# Does the output look like a somewhat reasonable continuation of the input text?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
